{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65328405-07f5-4095-84a8-7f6ed81af13d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e79fec-1d87-4df2-9ed7-db25cfe1d35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: google-auth in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.37.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.156.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth-oauthlib) (2.0.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client) (2.24.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (5.29.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.8.30)\n",
      "Google library is installed\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install pandas\n",
    "!pip install --upgrade google-auth google-auth-oauthlib google-api-python-client\n",
    "# Clear \n",
    "!python -c \"import google; print('Google library is installed')\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4abdf308-cf04-42fc-9f68-f981721bbe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MalikW\\AppData\\Local\\Programs\\Python\\Python313\\python.exe: No module named venv__proj_job_agg\n"
     ]
    }
   ],
   "source": [
    "!python -m venv__proj_job_agg .venv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2029862d-2eff-4029-a8fb-c36b808fd297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddace381-86b6-4f01-a513-54947e39c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-auth in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.37.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.155.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth-oauthlib) (2.0.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client) (2.24.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (5.29.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\malikw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0de6b7-82b1-43ca-b19c-626c31d78791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google library is installed\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55c0db79-6b34-4d95-9266-e1832a2d5460",
   "metadata": {},
   "source": [
    "# Google Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "929e4210-244f-418e-a6b2-4cafeee4adbc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=416298047261-pgh55f954cr71omfj22djkcudkjh8t36.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.modify&state=IAkRSEcC7Qy40VTmC9uOa1doNL7htB&access_type=offline\n",
      "Gmail API authenticated successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "\n",
    "# Define the scope for Gmail API\n",
    "SCOPES = ['https://www.googleapis.com/auth/gmail.modify']\n",
    "\n",
    "def authenticate_gmail():\n",
    "    \"\"\"Authenticate and get the Gmail API service\"\"\"\n",
    "    creds = None\n",
    "\n",
    "    # Check if token.pickle already exists for storing user credentials\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    # If no valid credentials, ask user to log in\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES\n",
    "            )\n",
    "            # Explicitly set redirect_uri for debugging\n",
    "            flow.redirect_uri = \"http://localhost:8080/\"\n",
    "            creds = flow.run_local_server(port=8080)\n",
    "        # Save credentials for next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    # Build the Gmail API service\n",
    "    return build('gmail', 'v1', credentials=creds)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Authenticate and initialize the Gmail API\n",
    "        service = authenticate_gmail()\n",
    "        print(\"Gmail API authenticated successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during authentication:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08089a-500f-469d-99e1-c3aac37ef144",
   "metadata": {},
   "source": [
    "# Sample email saved as html webpage \n",
    "So we can judge the struture and write the logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10f1f88e-3c7f-47c3-a377-fa420727fb9b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email saved as indeed_alert.html. Open this file in a browser to examine it.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "def fetch_email_and_save_as_html(service):\n",
    "    \"\"\"Fetch the top email from <alert@indeed.com> and save it as an HTML file.\"\"\"\n",
    "    try:\n",
    "        # Search query to find emails from <alert@indeed.com>\n",
    "        query = \"from:alert@indeed.com\"\n",
    "        results = service.users().messages().list(userId='me', q=query, maxResults=1).execute()\n",
    "\n",
    "        messages = results.get('messages', [])\n",
    "\n",
    "        if not messages:\n",
    "            print(\"No emails found from <alert@indeed.com>.\")\n",
    "            return\n",
    "\n",
    "        # Fetch the top email\n",
    "        message = service.users().messages().get(userId='me', id=messages[0]['id']).execute()\n",
    "\n",
    "        # Extract email body\n",
    "        payload = message['payload']\n",
    "        parts = payload.get('parts', [])\n",
    "\n",
    "        email_body = None\n",
    "\n",
    "        # Iterate through parts to find text/html\n",
    "        for part in parts:\n",
    "            mime_type = part.get('mimeType', '')\n",
    "            if mime_type == 'text/html' and 'body' in part and 'data' in part['body']:\n",
    "                email_body = decode_email_body(part['body']['data'])\n",
    "                break\n",
    "\n",
    "        if not email_body:\n",
    "            print(\"No HTML body found in the email.\")\n",
    "            return\n",
    "\n",
    "        # Save the email body as an HTML file\n",
    "        filename = \"indeed_alert.html\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(email_body)\n",
    "        print(f\"Email saved as {filename}. Open this file in a browser to examine it.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while fetching the email:\", e)\n",
    "\n",
    "\n",
    "def decode_email_body(encoded_body):\n",
    "    \"\"\"Decode base64 encoded email body.\"\"\"\n",
    "    try:\n",
    "        decoded_bytes = base64.urlsafe_b64decode(encoded_body)\n",
    "        return decoded_bytes.decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(\"Error decoding email body:\", e)\n",
    "        return \"Error decoding body.\"\n",
    "\n",
    "\n",
    "# Assuming `service` is already authenticated and available\n",
    "if __name__ == '__main__':\n",
    "    # Fetch the email and save it as an HTML file\n",
    "    fetch_email_and_save_as_html(service)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97f4d1-943f-4b47-b22d-727d1e7608c2",
   "metadata": {},
   "source": [
    "# Sample Fetching the data as needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63b4f7cf-0015-4e10-9383-820974e2f55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging <p> Tags:\n",
      "Raw HTML: <p align=\"center\" class=\"r-12\" style=\"font-family:'Noto Sans', Helvetica, Arial, sans-serif;font-size:14px;line-height:20px;color:#767676;Margin:0\">Refined by: within 100 kilometres, CIBC</p>\n",
      "Text Content: Refined by: within 100 kilometres, CIBC\n",
      "\n",
      "\n",
      "Raw HTML: <p align=\"center\" class=\"r-11\" style=\"font-family:'Noto Sans', Helvetica, Arial, sans-serif;font-size:14px;line-height:20px;color:#767676;Margin:0\">These job ads match your saved job alert ¹</p>\n",
      "Text Content: These job ads match your saved job alert ¹\n",
      "\n",
      "\n",
      "Raw HTML: <p align=\"center\" style=\"font-size:14px;color:#2D2D2D;Margin:0\">View jobs: <a href=\"https://ca.indeed.com/jobs?q=data+and+cibc&amp;hl=en&amp;from=ja&amp;l=Toronto%2C+ON&amp;radius=100&amp;alid=6760aa764a0da4183f947ebb&amp;sc=0kf%3Afcckey%282f81ab34c520110f%29%3B&amp;tmtk=1ifc3k1fog48n800&amp;utm_campaign=job_alerts&amp;utm_medium=email&amp;utm_source=jobseeker_emails&amp;of=1&amp;fr=b&amp;fromage=1\" style=\"color:#2557A7;font-size:14px;font-weight:normal;text-decoration:underline\">since yesterday</a> - <a href=\"https://ca.indeed.com/jobs?q=data+and+cibc&amp;hl=en&amp;from=ja&amp;l=Toronto%2C+ON&amp;radius=100&amp;alid=6760aa764a0da4183f947ebb&amp;sc=0kf%3Afcckey%282f81ab34c520110f%29%3B&amp;tmtk=1ifc3k1fog48n800&amp;utm_campaign=job_alerts&amp;utm_medium=email&amp;utm_source=jobseeker_emails&amp;of=1&amp;fr=b&amp;fromage=7\" style=\"color:#2557A7;font-size:14px;font-weight:normal;text-decoration:underline\">for last 7 days</a></p>\n",
      "Text Content: View jobs: since yesterday - for last 7 days\n",
      "\n",
      "\n",
      "Raw HTML: <p style=\"font-family:'Noto Sans', Helvetica, Arial, sans-serif;font-size:24px;line-height:28px;font-weight:bold;letter-spacing:0.06px;color:#2D2D2D;Margin:0\">Would you like to help other job seekers?</p>\n",
      "Text Content: Would you like to help other job seekers?\n",
      "\n",
      "\n",
      "Raw HTML: <p style=\"font-family:'Noto Sans', Helvetica, Arial, sans-serif;Margin:0;font-size:14px;font-weight:400;line-height:22px;letter-spacing:-0.12px;color:#000000\">Your review helps other job seekers determine if this company is the right fit.</p>\n",
      "Text Content: Your review helps other job seekers determine if this company is the right fit.\n",
      "\n",
      "\n",
      "\n",
      "Search Term: Search term not found\n",
      "\n",
      "Jobs Found:\n",
      "{'Job Title': 'View all jobs', 'Company': 'Vector Institute', 'Location': 'Unknown', 'Posted': 'Do not share this email'}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_job_details(html_file):\n",
    "    \"\"\"Extract job details like search term, company name, job title, location, and posting date.\"\"\"\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    # Debugging: Print all <p> tags\n",
    "    print(\"Debugging <p> Tags:\")\n",
    "    for p_tag in soup.find_all('p'):\n",
    "        print(\"Raw HTML:\", p_tag)\n",
    "        print(\"Text Content:\", p_tag.get_text())\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Extract Search Term\n",
    "    search_term = extract_search_term(soup)\n",
    "\n",
    "    jobs = []\n",
    "\n",
    "    # Extract job listings\n",
    "    job_listings = soup.find_all('a', style=lambda t: t and 'font-size:16px' in t)\n",
    "\n",
    "    for job in job_listings:\n",
    "        job_details = {}\n",
    "\n",
    "        # Job Title\n",
    "        job_details['Job Title'] = job.text.strip()\n",
    "\n",
    "        # Company Name\n",
    "        company_section = job.find_next('span', style=lambda t: t and 'font-weight:700' in t)\n",
    "        job_details['Company'] = company_section.text.strip() if company_section else 'Unknown'\n",
    "\n",
    "        # Location\n",
    "        location_section = company_section.find_next('span', style=lambda t: t and 'font-size:14px' in t) if company_section else None\n",
    "        job_details['Location'] = location_section.text.strip() if location_section else 'Unknown'\n",
    "\n",
    "        # Posting Date\n",
    "        posting_date_section = job.find_next('td', style=lambda t: t and 'font-family' in t)\n",
    "        job_details['Posted'] = posting_date_section.text.strip() if posting_date_section else 'Unknown'\n",
    "\n",
    "        jobs.append(job_details)\n",
    "\n",
    "    return search_term, jobs\n",
    "\n",
    "\n",
    "def extract_search_term(soup):\n",
    "    \"\"\"Extract the search term from the email.\"\"\"\n",
    "    # Search for the paragraph containing the text directly\n",
    "    alert_paragraph = soup.find(string=lambda text: 'Your job alert is active' in text if text else False)\n",
    "    \n",
    "    if alert_paragraph:\n",
    "        # Check for sibling or child elements containing the search term\n",
    "        parent = alert_paragraph.find_parent()\n",
    "        if parent:\n",
    "            bold_tags = parent.find_all('b')\n",
    "            if bold_tags and len(bold_tags) >= 1:\n",
    "                return bold_tags[0].get_text().strip()\n",
    "\n",
    "    return \"Search term not found\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Path to your Indeed email HTML file\n",
    "    html_file = 'indeed_alert.html'\n",
    "    search_term, jobs = extract_job_details(html_file)\n",
    "\n",
    "    print(f\"\\nSearch Term: {search_term}\\n\")\n",
    "    print(\"Jobs Found:\")\n",
    "    for job in jobs:\n",
    "        print(job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5297a5af-f85f-4f8c-88a9-fec81280702e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Job Details:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Job details saved to cleaned_job_alerts_with_dates_and_days.csv\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def extract_job_details(html_file):\n",
    "    \"\"\"Extract and clean job details.\"\"\"\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    # Extract Search Term\n",
    "    search_term = extract_search_term(soup)\n",
    "\n",
    "    # Extract Refined By\n",
    "    refined_by = extract_refined_by(soup)\n",
    "\n",
    "    jobs = []\n",
    "    script_execution_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Extract Job Listings\n",
    "    job_listings = soup.find_all('a', href=True, style=lambda x: x and 'font-size:16px' in x)\n",
    "\n",
    "    for job in job_listings:\n",
    "        job_details = {}\n",
    "\n",
    "        # Job Title\n",
    "        job_title = job.text.strip()\n",
    "        if not job_title or \"View all jobs\" in job_title or \"Edit\" in job_title:\n",
    "            continue  # Skip rows with unwanted titles\n",
    "\n",
    "        # Company Name\n",
    "        company_section = job.find_next('span', class_='r-g')\n",
    "        company = company_section.text.strip() if company_section else \"Unknown\"\n",
    "        company = re.sub(r'\\s*\\d+\\.\\d+\\s*', '', company)  # Remove ratings like \"3.9\"\n",
    "\n",
    "        # Location\n",
    "        location_section = job.find_next('span', class_='r-h')\n",
    "        location = location_section.text.replace('-', '').strip() if location_section else \"Unknown\"\n",
    "\n",
    "        # Posted\n",
    "        posted_section = job.find_next('td', class_='r-k')\n",
    "        posted = posted_section.text.strip() if posted_section else \"Unknown\"\n",
    "\n",
    "        # Calculate Date of Post\n",
    "        date_of_post = calculate_date_of_post(posted)\n",
    "        day_of_week = calculate_day_of_week(date_of_post)\n",
    "\n",
    "        # Add Data\n",
    "        job_details['Search Term'] = search_term\n",
    "        job_details['Refined By'] = refined_by\n",
    "        job_details['Job Title'] = job_title\n",
    "        job_details['Company'] = company\n",
    "        job_details['Location'] = location\n",
    "        job_details['Days Since Posted'] = posted\n",
    "        job_details['Date of Post'] = date_of_post\n",
    "        job_details['Day'] = day_of_week\n",
    "        job_details['Script Execution Date'] = script_execution_date\n",
    "\n",
    "        jobs.append(job_details)\n",
    "\n",
    "    return jobs\n",
    "\n",
    "\n",
    "def extract_search_term(soup):\n",
    "    \"\"\"Extract search term dynamically from the entire HTML text.\"\"\"\n",
    "    try:\n",
    "        full_text = soup.get_text(separator=\" \", strip=True)\n",
    "        match = re.search(r\"for\\s+(.*?)\\s+in\\s+\", full_text)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return \"Search term not found\"\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting search term:\", e)\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "def extract_refined_by(soup):\n",
    "    \"\"\"Extract 'Refined By' details.\"\"\"\n",
    "    try:\n",
    "        refined_section = soup.find('p', string=lambda text: text and \"Refined by\" in text)\n",
    "        if refined_section:\n",
    "            return refined_section.text.replace(\"Refined by: \", \"\").strip()\n",
    "        return \"Refined by not found\"\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting refined by:\", e)\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "def calculate_date_of_post(posted_text):\n",
    "    \"\"\"Calculate the exact date based on the 'Posted' text.\"\"\"\n",
    "    today = datetime.now()\n",
    "    if \"day\" in posted_text:\n",
    "        days_ago = int(re.search(r'\\d+', posted_text).group())\n",
    "        return (today - timedelta(days=days_ago)).strftime(\"%Y-%m-%d\")\n",
    "    elif \"hour\" in posted_text:\n",
    "        hours_ago = int(re.search(r'\\d+', posted_text).group())\n",
    "        return (today - timedelta(hours=hours_ago)).strftime(\"%Y-%m-%d\")\n",
    "    elif \"week\" in posted_text:\n",
    "        weeks_ago = int(re.search(r'\\d+', posted_text).group())\n",
    "        return (today - timedelta(weeks=weeks_ago)).strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "def calculate_day_of_week(date_string):\n",
    "    \"\"\"Calculate the day of the week based on a date string.\"\"\"\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
    "        return date_obj.strftime(\"%A\")\n",
    "    except Exception:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    html_file = 'indeed_alert.html'\n",
    "\n",
    "    # Extract job details\n",
    "    jobs = extract_job_details(html_file)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(jobs)\n",
    "\n",
    "    print(\"\\nCleaned Job Details:\")\n",
    "    print(df)\n",
    "\n",
    "    # Save to CSV\n",
    "    output_file = \"cleaned_job_alerts_with_dates_and_days.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nJob details saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfe425-9adc-4099-a300-280aac23a25d",
   "metadata": {},
   "source": [
    "# **Explanation of Parsing Logic**\n",
    "\n",
    "The script uses **BeautifulSoup** to parse and analyze the structure of an HTML email. Parsing involves reading through the HTML, identifying key components, and extracting the desired information.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Reading the HTML File**\n",
    "- The script reads the raw HTML content of the Indeed email using **BeautifulSoup**.  \n",
    "- **HTML content** is broken into a tree-like structure of nested tags (e.g., `<p>`, `<span>`, `<a>`, etc.) that represent elements like job titles, company names, and search filters.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Finding Specific Tags and Elements**\n",
    "\n",
    "To extract data, the script uses **tag searching** and **selectors**:\n",
    "\n",
    "### **a. Job Listings**  \n",
    "- **Target Element**: Job titles are located inside `<a>` tags with specific inline styles (`font-size:16px`).\n",
    "- **Search Method**:  \n",
    "   The script uses BeautifulSoup to search for all `<a>` tags where the style matches the expected pattern.  \n",
    "   This isolates the clickable links that represent job titles.\n",
    "\n",
    "### **b. Company Name**  \n",
    "- **Target Element**: Company names are usually stored in `<span>` tags with specific classes (e.g., `r-g`).\n",
    "- **Search Method**:  \n",
    "   Once a job title is found, the script searches for the **next sibling `<span>` tag**.  \n",
    "   This is based on the assumption that company names appear immediately after the job title in the HTML structure.\n",
    "\n",
    "### **c. Location**  \n",
    "- **Target Element**: Locations are often stored in `<span>` tags or inline elements immediately following the company name.  \n",
    "- **Search Method**:  \n",
    "   The script looks for the **next sibling `<span>`** after the company name.  \n",
    "   If a dash (`-`) is present, the script cleans the text to isolate the location (e.g., \"Toronto, ON\").\n",
    "\n",
    "### **d. Days Since Posted**  \n",
    "- **Target Element**: Posting dates (e.g., \"2 days ago\") are located in `<td>` tags with specific classes like `r-k`.\n",
    "- **Search Method**:  \n",
    "   After finding the job title and company, the script navigates to the **next `<td>`** element to extract this information.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Extracting Search Term**\n",
    "The **search term** is extracted dynamically by scanning the HTML content for specific text patterns:\n",
    "\n",
    "- **Target Sentence**: `\"You'll receive your first daily job alert for ... in Toronto, ON.\"`\n",
    "- **Method**:\n",
    "   - The script searches through the entire HTML as plain text.\n",
    "   - Using **regex**, it identifies and extracts the portion of the sentence between `\"for\"` and `\"in\"`.  \n",
    "     For example:\n",
    "     - **Sentence**: \"You'll receive your first daily job alert for data and rbc in Toronto, ON.\"\n",
    "     - **Extracted Term**: `data and rbc`\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Parsing Relative Dates**\n",
    "Relative posting times like **\"2 days ago\"** or **\"3 weeks ago\"** are dynamically parsed to compute exact dates.\n",
    "\n",
    "- **Method**:\n",
    "   - The script identifies the relative time using patterns like:\n",
    "      - `\\d+ day(s) ago` → Extracts \"2\" from \"2 days ago\".\n",
    "   - Subtracts the corresponding number of days or weeks from today’s date using Python’s `datetime` library.\n",
    "\n",
    "For example:\n",
    "- **Input**: \"2 days ago\"\n",
    "- **Today's Date**: 2024-06-16\n",
    "- **Result**: 2024-06-14\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Cleaning and Processing Text**\n",
    "HTML data often contains unwanted content like:\n",
    "- Extra spaces\n",
    "- Ratings (e.g., \"3.9\") included with company names\n",
    "- Unnecessary symbols like dashes (`-`)\n",
    "\n",
    "**Method**:\n",
    "- The script uses **regular expressions (regex)** and string methods to clean the text:\n",
    "   - Remove ratings: `Royal Bank of Canada 3.9` → `Royal Bank of Canada`\n",
    "   - Remove extra dashes: `\" - Toronto, ON\"` → `\"Toronto, ON\"`\n",
    "   - Strip extra spaces and line breaks.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Navigating Between Tags**\n",
    "HTML elements are often nested or sequentially structured. The script uses **navigation techniques** to move between tags:\n",
    "\n",
    "- **Next Sibling**:  \n",
    "   For example, after finding a job title, the script uses `find_next()` to locate the **company name** and then the **location**.\n",
    "- **Tag Attributes**:  \n",
    "   Tags like `<a>` or `<span>` often have unique attributes (e.g., `style` or `class`) that the script uses to precisely identify the correct elements.\n",
    "- **Text Content**:  \n",
    "   Once the correct tag is located, the script extracts only the textual content inside it (ignoring other nested HTML tags).\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Summary of Parsing Logic**\n",
    "The script effectively breaks the HTML into its components, navigates through the structure, and extracts the required information:\n",
    "\n",
    "1. **Job Titles** → `<a>` tags with styles.\n",
    "2. **Company Names** → Next `<span>` tag with a specific class.\n",
    "3. **Location** → Next `<span>` tag after the company name.\n",
    "4. **Posted Dates** → `<td>` elements with specific classes.\n",
    "5. **Search Term** → Extracted using regex from the entire text content.\n",
    "6. **Text Cleaning** → Regular expressions remove noise like ratings and unnecessary characters.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why It Works**\n",
    "- The script uses a combination of **structured navigation** (moving between tags) and **text parsing** (regex) to extract data.\n",
    "- It is **robust**: If one tag fails, the script falls back on defaults like `\"Unknown\"`.\n",
    "- It works dynamically with both relative dates (\"2 days ago\") and plain text patterns.\n",
    "\n",
    "This approach ensures clean, structured, and enriched data is extracted from even messy or inconsistent HTML.\n",
    "\n",
    "Let me know if you need further clarifications on any specific part! 🚀\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88477b17-febc-4bb2-92e1-35ef288fdb69",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e1003-4708-4a82-8ee4-59b5a6c1a5bb",
   "metadata": {},
   "source": [
    "# On email recive trigger the parser - Active Listner\n",
    "This listens to all the new eamil from a spcified sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c561e051-4000-4a9c-b037-584090a02bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gmail API authenticated successfully!\n",
      "No new emails found.\n",
      "Waiting for new emails...\n",
      "No new emails found.\n",
      "Waiting for new emails...\n",
      "No new emails found.\n",
      "Waiting for new emails...\n",
      "No new emails found.\n",
      "Waiting for new emails...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m         list_new_emails(service)\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for new emails...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Poll every 10 seconds\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# Define the sender and query for filtering\n",
    "SEARCH_SENDER = \"malikhq27@gmail.com\"\n",
    "\n",
    "def list_new_emails(service):\n",
    "    \"\"\"List new emails from a specific sender and print their titles.\"\"\"\n",
    "    try:\n",
    "        # Search for emails matching the query (e.g., from:alert@indeed.com)\n",
    "        query = f\"from:{SEARCH_SENDER} is:unread\"\n",
    "        results = service.users().messages().list(userId='me', q=query).execute()\n",
    "        messages = results.get('messages', [])\n",
    "\n",
    "        if not messages:\n",
    "            print(\"No new emails found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Found {len(messages)} new email(s) from {SEARCH_SENDER}.\")\n",
    "        for message in messages:\n",
    "            # Fetch email details\n",
    "            msg = service.users().messages().get(userId='me', id=message['id']).execute()\n",
    "            headers = msg['payload']['headers']\n",
    "\n",
    "            # Extract subject from email headers\n",
    "            subject = next((header['value'] for header in headers if header['name'] == 'Subject'), \"No Subject\")\n",
    "            print(f\"New Email Title: {subject}\")\n",
    "\n",
    "            # Mark the email as read (optional)\n",
    "            service.users().messages().modify(\n",
    "                userId='me',\n",
    "                id=message['id'],\n",
    "                body={\"removeLabelIds\": [\"UNREAD\"]}\n",
    "            ).execute()\n",
    "\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Authenticate using Gmail API\n",
    "        service = authenticate_gmail()\n",
    "        print(\"Gmail API authenticated successfully!\")\n",
    "\n",
    "        # Continuously check for new emails\n",
    "        while True:\n",
    "            list_new_emails(service)\n",
    "            print(\"Waiting for new emails...\")\n",
    "            time.sleep(10)  # Poll every 10 seconds\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962bfb74-fa7a-4cde-b2d8-5ebcdd88c63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee71f1-d623-4396-99ea-97a5b6f109ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353593f1-9cbd-4f67-89b0-a8af6c21f948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022acbe-904a-4b4c-b91f-3bc3c115cea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
